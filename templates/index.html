{% extends "base.html" %}
{% block content %}

{% set model = config.get('model', {}) if config else {} %}

<h1>LoRA Trainer</h1>

<form method="get">
    <label>Project:</label>
    <select name="project" onchange="this.form.submit()">
        <option value="">-- select --</option>
        {% for p in projects %}
            <option value="{{ p }}" {% if p == selected %}selected{% endif %}>{{ p }}</option>
        {% endfor %}
    </select>
</form>

<div id="theme-menu">
  <select id="theme-select">
    <option value="dark">Dark</option>
    <option value="gray">Gray</option>
    <option value="slate">Slate</option>
    <option value="midnight">Midnight</option>
  </select>
</div>

<form method="post" action="{{ url_for('projects.create_project_route') }}" style="margin-top: 16px;">
  <label>
    New Project
    <span class="tooltip"
          data-tip="Creates a new LoRA project folder with default configuration.">
      ⓘ
    </span>
  </label><br>

  <input type="text"
         name="project_name"
         placeholder="Project name"
         required>

  <button type="submit" style="margin-left: 8px;">
    Create
  </button>
</form>

{% if selected %}
<div class="section danger-zone">
  <div class="danger-title">Danger Zone</div>

  <form method="post" action="{{ url_for('projects.delete_project_route') }}">
    <input type="hidden" name="project_name" value="{{ selected }}">

    <label>
      Type <strong>{{ selected }}</strong> to delete this project
      <span class="tooltip"
            data-tip="This permanently deletes the project folder and config. This cannot be undone.">
        ⓘ
      </span>
    </label><br>

    <input name="confirm_name" type="text" placeholder="Project name">
    <br><br>

    <button type="submit" class="danger">
      Delete Project
    </button>
  </form>
</div>
{% endif %}

{% if config %}
<hr>

<form method="post">
<input type="hidden" name="project" value="{{ selected }}">

{% set dataset = config.get('dataset', {}) %}
{% set captions = dataset.get('captions', {}) %}
{% set bucket = dataset.get('bucket', {}) %}
{% set training = config.get('training', {}) %}
{% set lrs = training.get('learning_rates', {}) %}
{% set cond = training.get('conditioning', {}) %}
{% set lora = config.get('lora', {}) %}
{% set precision = config.get('precision', {}) %}
{% set optimizer = config.get('optimizer', {}) %}
{% set betas = optimizer.get('betas', []) %}
{% set scheduler = config.get('scheduler', {}) %}

<div class="section">
<h2>Model</h2>

<label>
  Architecture
  <span class="tooltip"
        data-tip="Select the base model this LoRA is trained for. SDXL requires more VRAM than SD 1.5.">
    ⓘ
  </span>
</label><br>

<select name="model_architecture">
  <option value="sdxl"
    {% if model.get('architecture', 'sdxl') == "sdxl" %}selected{% endif %}>
    SDXL
  </option>
  <option value="sd15"
    {% if model.get('architecture') == "sd15" %}selected{% endif %}>
    SD 1.5
  </option>
</select>

<br><br>

<label>
  Base Model
  <span class="tooltip"
        data-tip="Select a base model from the models folder. Leave empty to use the default.">
    ⓘ
  </span>
</label><br>

<select name="model_checkpoint">
  <option value="">-- use default --</option>

  {% for m in available_models %}
    <option value="{{ m }}"
      {% if model.get('checkpoint') == m %}selected{% endif %}>
      {{ m }}
    </option>
  {% endfor %}
</select>

<!-- ===================== DATASET ===================== -->

<div class="section">
<h2>Dataset</h2>

<label>Dataset Path</label><br>
<input name="dataset_path" type="text" value="{{ dataset.get('path','') }}">
<br><br>

<label>
  Resolution
  <span class="tooltip"
        data-tip="Images are automatically resized during training. 
        Mixed image sizes will not cause errors, but inconsistent framing may reduce training result quality. 
        For best results, use similarly framed images or crop your dataset before training.">
    ⓘ
  </span>
</label><br>
<input name="resolution" type="number" value="{{ dataset.get('resolution','') }}">
<br><br>

<label>Repeats</label><br>
<input
  type="number"
  id="repeats"
  name="repeats"
  min="1"
  value="{{ config.dataset.repeats }}"
>
<br><br>

<label>
  Batch Size
  <span class="tooltip"
        data-tip="Number of images processed per training step. Higher values use more VRAM.">
    ⓘ
  </span>
</label><br>
<input name="batch_size" type="number"
       value="{{ dataset.get('batch_size','') }}"
       class="{% if 'batch_size' in danger_fields %}danger{% endif %}">
<br><br>

<label>
  <input type="checkbox" name="shuffle" {% if dataset.get('shuffle') %}checked{% endif %}>
  Shuffle Dataset
</label>
<br><br>

<label>
  <input type="checkbox" name="cache_latents" {% if dataset.get('cache_latents') %}checked{% endif %}>
  Cache Latents
</label>

<hr>

<h3>Captions</h3>

<label>Extension</label><br>
<input name="caption_extension" type="text" value="{{ captions.get('extension','') }}">
<br><br>

<label>
  <input type="checkbox" name="first_word_memorize"
         {% if captions.get('first_word_memorize') %}checked{% endif %}>
  Memorize First Caption Word
</label>
<br><br>

<label>
  Prepend Token
  <span class="tooltip" data-tip="
Token added to the beginning of every caption.
Often used as a trigger word for the LoRA.
Example: <my_character>
*Currently Unsupported*
">ⓘ</span>
</label><br>
<input name="prepend_token" type="text" value="{{ captions.get('prepend_token','') }}">
<br><br>

<label>
  Append Token
  <span class="tooltip" data-tip="
Token added to the end of every caption.
Useful for enforcing styles or modifiers.
Less commonly used than prepend tokens.
*Currently Unsupported*
">ⓘ</span>
</label><br>
<input name="append_token" type="text" value="{{ captions.get('append_token','') }}">

<hr>

<h3>Bucketing</h3>

<label>
  <input type="checkbox" name="bucket_enabled"
         {% if bucket.get('enabled') %}checked{% endif %}>
  Enable Bucketing
  <span class="tooltip" data-tip="
Groups images by similar resolutions.
Reduces distortion for mixed-resolution datasets.
Does not upscale images.
">ⓘ</span>
</label>
<br><br>

<label>Min Resolution</label><br>
<input name="bucket_min_res" type="number" value="{{ bucket.get('min_res','') }}">
<br><br>

<label>Max Resolution</label><br>
<input name="bucket_max_res" type="number" value="{{ bucket.get('max_res','') }}">
<br><br>

<label>
  Step
  <span class="tooltip" data-tip="
Resolution step size used for bucketing.
Common values: 32 or 64.
Smaller values increase flexibility but cost more VRAM.
">ⓘ</span>
</label><br>
<input name="bucket_step" type="number" value="{{ bucket.get('step','') }}">

<hr>
</div>

<!-- ===================== TRAINING ===================== -->

<div class="section">
<h2>Training</h2>

<label>
  Epochs
  <span class="tooltip"
        data-tip="Number of full passes over the dataset. More epochs increase training time and risk of overfitting.">
    ⓘ
  </span>
</label><br>
<input name="epochs" type="number"
       value="{{ training.get('epochs','') }}"
       class="{% if 'epochs' in danger_fields %}danger{% endif %}">
<br><br>

<label>
  Save Every N Epochs
  <span class="tooltip"
        data-tip="Controls how often checkpoints are saved during training. Example: 1 saves every epoch, 5 saves every 5 epochs.">
    ⓘ
  </span>
</label><br>

<input
  name="save_every_epochs"
  type="number"
  min="1"
  value="{{ training.get('save_every_epochs', 1) }}"
>
<br><br>

<label>
  Run inference previews
</label><br>
<input type="checkbox" name="do_inference"
       {% if training.get("do_inference") %}checked{% endif %}>
<br><br>

<label>
  Gradient Accumulation
  <span class="tooltip" data-tip="
Simulates a larger batch size by accumulating gradients across steps.
Batch Size 1 + Accumulation 4 = Effective batch size 4.
Reduces VRAM usage but slows training.
">ⓘ</span>
</label><br>
<input name="gradient_accumulation" type="number"
       value="{{ training.get('gradient_accumulation','') }}">
<br><br>

<label>
  CLIP Skip
  <span class="tooltip"
        data-tip="Skips early CLIP layers. Higher values reduce literal prompt matching and can weaken concept learning.">
    ⓘ
  </span>
</label><br>
<input name="clip_skip" type="number"
       value="{{ cond.get('clip_skip','') }}">
<br><br>

<h3>Learning Rates</h3>

<label>
  UNet LR
  <span class="tooltip"
        data-tip="Learning rate for UNet weights. Typical range is 1e-5 to 5e-5. Too high can destroy training.
        
        If training loss becomes NaN, the run is invalid and will not recover.
Common causes:
- Learning rate too high
- Mixed precision instability (fp16 / bf16)

Fix by lowering LR or changing precision.
">
    ⓘ
  </span>
</label><br>
<input name="lr_unet" type="text"
       value="{{ lrs.get('unet','') }}"
       class="{% if 'lr_unet' in danger_fields %}danger{% endif %}">
<br><br>

<label>
  CLIP LR
  <span class="tooltip"
        data-tip="Learning rate for CLIP text encoder. Usually equal to or lower than UNet LR. Higher values can harm prompt understanding.">
    ⓘ
  </span>
</label><br>
<input name="lr_clip" type="text"
       value="{{ lrs.get('clip','') }}"
       class="{% if 'lr_clip' in danger_fields %}danger{% endif %}">
<br><br>

<hr>
</div>

<!-- ===================== LoRA ===================== -->

<div class="section">
<h2>LoRA</h2>

<label>
  Rank
  <span class="tooltip"
        data-tip="Controls LoRA capacity. Higher values increase file size and training time. Common values are 8, 16, 32, or 64.">
    ⓘ
  </span>
</label><br>
<input name="lora_rank" type="number" value="{{ lora.get('rank','') }}">
<br><br>

<label>Alpha</label><br>
<input name="lora_alpha" type="number" value="{{ lora.get('alpha','') }}">
<br><br>

<label>
  Dropout
  <span class="tooltip" data-tip="
Randomly disables a portion of LoRA weights during training.
Helps reduce overfitting on small datasets.
Most LoRA training uses 0.0.
">ⓘ</span>
</label><br>
<input name="lora_dropout" type="text" value="{{ lora.get('dropout','') }}">
<br><br>

<label>
  Target Modules
  <span class="tooltip" data-tip="
Specifies which layers LoRA is applied to.
Leaving this empty uses recommended defaults.
Advanced option — usually unnecessary.
">ⓘ</span>
</label><br>
<input name="lora_target_modules" type="text" value="{{ lora.get('target_modules','') }}">
<br><br>

<hr>
</div>

<!-- ===================== PRECISION ===================== -->

<div class="section">
<h2>Precision</h2>

<label>Mixed Precision</label><br>
<select name="mixed_precision">
{% for opt in ["fp16","bf16","fp32"] %}
  <option value="{{ opt }}" {% if precision.get('mixed_precision') == opt %}selected{% endif %}>{{ opt }}</option>
{% endfor %}
</select>
<br><br>

<label>
  <input type="checkbox" name="gradient_checkpointing"
         {% if precision.get('gradient_checkpointing') %}checked{% endif %}>
  Gradient Checkpointing
  <span class="tooltip" data-tip="
Reduces VRAM usage by recomputing activations.
Slows training but greatly lowers memory usage.
Does not affect final quality.
">ⓘ</span>
</label>
<br><br>

<label>
  <input type="checkbox" name="xformers"
         {% if precision.get('xformers') %}checked{% endif %}>
  Use xFormers
</label>
<br><br>

<label>
  <input type="checkbox" name="cpu_offload"
         {% if precision.get('cpu_offload') %}checked{% endif %}>
  CPU Offload
</label>

<hr>
</div>

<!-- ===================== OPTIMIZER ===================== -->

<div class="section">
<h2>Optimizer</h2>

<label>Optimizer Type</label><br>
<select name="optimizer_type">
{% for opt in ["adamw","adam","sgd"] %}
  <option value="{{ opt }}" {% if optimizer.get('type') == opt %}selected{% endif %}>{{ opt }}</option>
{% endfor %}
</select>
<br><br>

<label>
  Weight Decay
  <span class="tooltip" data-tip="
Penalizes large weights to reduce overfitting.

Acts like regularization — higher values encourage simpler models but can weaken learning.
Typical values are 0.0 – 0.1.

Most LoRA training works fine with 0 or very small values.
  ">ⓘ</span>
</label><br>
<input name="weight_decay" type="text" value="{{ optimizer.get('weight_decay','') }}">
<br><br>

<label>
  Beta 1
  <span class="tooltip" data-tip="
Controls how much the optimizer remembers recent gradients.

Higher values smooth training but react slower to changes.
Typical value: 0.9.

Rarely needs adjustment for LoRA training.
  ">ⓘ</span>
</label><br>
<input name="beta1" type="text" value="{{ betas[0] if betas|length > 0 else '' }}">
<br><br>

<label>
  Beta 2
  <span class="tooltip" data-tip="
Controls how much the optimizer remembers long-term gradient variance.

Higher values make training more stable but slower to adapt.
Typical value: 0.999.

Usually safe to leave at default.
  ">ⓘ</span>
</label><br>
<input name="beta2" type="text" value="{{ betas[1] if betas|length > 1 else '' }}">
<br><br>

<label>
  Epsilon
  <span class="tooltip" data-tip="
Small value added for numerical stability.

Prevents division by zero inside the optimizer.
Typical values are 1e-8 to 1e-6.

Changing this rarely improves results.
  ">ⓘ</span>
</label><br>
<input name="epsilon" type="text" value="{{ optimizer.get('epsilon','') }}">

<hr>
</div>

<!-- ===================== SCHEDULER ===================== -->

<div class="section">
<h2>Scheduler</h2>

<label>
  Scheduler Type
  <span class="tooltip" data-tip="
Controls how learning rate changes during training.
cosine: smooth decay - 
linear: steady decay - 
constant: no decay
">ⓘ</span>
</label><br>
<select name="scheduler_type">
{% for opt in ["cosine","linear","constant"] %}
  <option value="{{ opt }}" {% if scheduler.get('type') == opt %}selected{% endif %}>{{ opt }}</option>
{% endfor %}
</select>
<br><br>

<label>Warmup Steps</label><br>
<input name="warmup_steps" type="number" value="{{ scheduler.get('warmup_steps','') }}">
<br><br>

<label>
  Num Cycles
  <span class="tooltip" data-tip="
Used by some schedulers (e.g. cosine).
Controls how many LR cycles occur.
Most setups use 1.
">ⓘ</span>
</label><br>
<input name="num_cycles" type="number" value="{{ scheduler.get('num_cycles','') }}">

<hr>
</div>

<!-- ===================== TRAINING ===================== -->

<div class="section">
  <h2>Training</h2>

  <p style="color: var(--muted); margin-bottom: 12px;">
    Uses the current configuration to start LoRA training.
  </p>

  <button
      type="submit"
      name="action"
      value="train"
      class="train-button"
      formaction="{{ url_for('training.start_training', project=selected) }}"
      {% if has_fatal %}disabled{% endif %}>
      Start Training
  </button>

</div>

<!-- ===================== STOP ===================== -->

{% if training_status == "running" %}
<div class="section">
  <h2>Training Logs</h2>
  
  <p style="color: var(--muted);">
    Training is currently running.
  </p>

  <pre id="log-box" style="max-height: 300px; overflow-y: auto;"></pre>

  <button
      type="submit"
      name="action"
      value="stop"
      class="stop-button"
      formaction="{{ url_for('training.stop_training', project=selected) }}">
      Stop Training
  </button>

</div>

<!-- ===================== LIVE STATUS ===================== -->

<script>
async function fetchLogs() {
  const res = await fetch("/train_logs/{{ selected }}");
  const data = await res.json();
  const box = document.getElementById("log-box");
  box.textContent = data.logs;
  box.scrollTop = box.scrollHeight;
}

fetchLogs();
setInterval(fetchLogs, 2000);
</script>
{% endif %}

{% set fatal_issues = issues | selectattr("level", "equalto", "fatal") | list %}
{% set has_fatal = fatal_issues | length > 0 %}
{% set has_danger = danger_fields | length > 0 %}

<!-- ===================== SAVE ===================== -->

{% if issues %}
  <div class="warning-box">
    {% for i in issues %}
      <div>{{ i.message }}</div>
    {% endfor %}
  </div>
{% endif %}

{% if has_fatal %}
  {# Fatal error: cannot proceed, only allow Save so user can correct config #}
  <button type="submit">Save</button>

{% elif has_danger %}
  {# Risky but allowed: user may proceed #}
  <button name="confirm" value="yes">Proceed Anyway</button>
  <button type="submit">Cancel</button>

{% else %}
  <button type="submit">Save</button>
{% endif %}

<script>
  // Save scroll position when a submit action is triggered
  document.addEventListener("click", (e) => {
    const btn = e.target.closest("button");
    if (!btn) return;

    if (btn.type === "submit") {
      localStorage.setItem("scrollY", window.scrollY);
    }
  });

  // Restore scroll position after page load
  window.addEventListener("load", () => {
    const y = localStorage.getItem("scrollY");
    if (y !== null) {
      window.scrollTo(0, parseInt(y, 10));
    }
  });
</script>

<script>
const themes = {
  dark: "#0f1115",
  gray: "#1e1e1e",
  slate: "#1b2028",
  midnight: "#0a0c10",
};

const select = document.getElementById("theme-select");

function applyTheme(name) {
  document.documentElement.style.setProperty("--bg", themes[name]);
  localStorage.setItem("ui_theme", name);
}

const saved = localStorage.getItem("ui_theme") || "dark";
select.value = saved;
applyTheme(saved);

select.addEventListener("change", e => applyTheme(e.target.value));
</script>

{% endif %}
{% endblock %}
